{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f5e393c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import Normalize\n",
    "from matplotlib.gridspec import GridSpec\n",
    "from matplotlib.patches import Circle\n",
    "import tf_silent\n",
    "from pinn import PINN\n",
    "from network import Network\n",
    "from optimizer_sep_losses import L_BFGS_B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c8f75b1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing model with cylinder weight: 5.0\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "SavedModel file does not exist at: model_data/track_sep_losses/eqn5_cyl5/implicit_noroi_LC_eqn5cyl5.keras/{saved_model.pbtxt|saved_model.pb}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-11cd7155118a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    122\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    123\u001b[0m \u001b[1;31m# Run the analysis\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 124\u001b[1;33m \u001b[0menhanced_results\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0manalyze_all_models\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-3-11cd7155118a>\u001b[0m in \u001b[0;36manalyze_all_models\u001b[1;34m()\u001b[0m\n\u001b[0;32m     96\u001b[0m         \u001b[1;31m# Load model and predict\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m         \u001b[1;31m# from tensorflow.keras.models import load_model # tf.keras.models.load_model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 98\u001b[1;33m         \u001b[0mnetwork\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     99\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m         \u001b[1;31m# Create inference grid\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\anaconda\\envs\\pinnpy3.6\\lib\\site-packages\\tensorflow_core\\python\\keras\\saving\\save.py\u001b[0m in \u001b[0;36mload_model\u001b[1;34m(filepath, custom_objects, compile)\u001b[0m\n\u001b[0;32m    147\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    148\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstring_types\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 149\u001b[1;33m     \u001b[0mloader_impl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparse_saved_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    150\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0msaved_model_load\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\anaconda\\envs\\pinnpy3.6\\lib\\site-packages\\tensorflow_core\\python\\saved_model\\loader_impl.py\u001b[0m in \u001b[0;36mparse_saved_model\u001b[1;34m(export_dir)\u001b[0m\n\u001b[0;32m     81\u001b[0m                   (export_dir,\n\u001b[0;32m     82\u001b[0m                    \u001b[0mconstants\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSAVED_MODEL_FILENAME_PBTXT\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 83\u001b[1;33m                    constants.SAVED_MODEL_FILENAME_PB))\n\u001b[0m\u001b[0;32m     84\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m: SavedModel file does not exist at: model_data/track_sep_losses/eqn5_cyl5/implicit_noroi_LC_eqn5cyl5.keras/{saved_model.pbtxt|saved_model.pb}"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.signal import find_peaks\n",
    "from scipy.interpolate import interp1d\n",
    "\n",
    "def enhanced_wake_metrics(x_grid, y_grid, u, v, p, model_name=\"\"):\n",
    "    \"\"\"\n",
    "    Comprehensive wake region analysis for Kármán vortices detection\n",
    "    \"\"\"\n",
    "    # Define wake region (adjust as needed)\n",
    "    wake_x_start = 0.6  # just behind cylinder\n",
    "    wake_x_end = 1.5\n",
    "    wake_y_center = 0.5\n",
    "    wake_y_half_width = 0.3\n",
    "    \n",
    "    # Create wake mask\n",
    "    wake_mask = ((x_grid >= wake_x_start) & (x_grid <= wake_x_end) & \n",
    "                 (y_grid >= wake_y_center - wake_y_half_width) & \n",
    "                 (y_grid <= wake_y_center + wake_y_half_width))\n",
    "    \n",
    "    metrics = {}\n",
    "    \n",
    "    # 1. Minimum velocity in wake (your current metric)\n",
    "    u_wake = u[wake_mask]\n",
    "    metrics['min_u_wake'] = float(np.min(u_wake))\n",
    "    metrics['mean_u_wake'] = float(np.mean(u_wake))\n",
    "    \n",
    "    # 2. Vorticity magnitude in wake region\n",
    "    # Calculate vorticity: ω = ∂v/∂x - ∂u/∂y\n",
    "    dy = y_grid[1, 0] - y_grid[0, 0]\n",
    "    dx = x_grid[0, 1] - x_grid[0, 0]\n",
    "    \n",
    "    du_dy = np.gradient(u, dy, axis=0)\n",
    "    dv_dx = np.gradient(v, dx, axis=1)\n",
    "    vorticity = dv_dx - du_dy\n",
    "    \n",
    "    vorticity_wake = vorticity[wake_mask]\n",
    "    metrics['max_vorticity_magnitude'] = float(np.max(np.abs(vorticity_wake)))\n",
    "    metrics['mean_vorticity_magnitude'] = float(np.mean(np.abs(vorticity_wake)))\n",
    "    \n",
    "    # 3. Velocity fluctuation analysis (indicator of unsteady behavior)\n",
    "    # Check for alternating positive/negative regions\n",
    "    centerline_idx = np.argmin(np.abs(y_grid[:, 0] - 0.5))\n",
    "    u_centerline = u[centerline_idx, :]\n",
    "    x_centerline = x_grid[centerline_idx, :]\n",
    "    \n",
    "    # Focus on wake region along centerline\n",
    "    wake_centerline_mask = (x_centerline >= wake_x_start) & (x_centerline <= wake_x_end)\n",
    "    u_wake_centerline = u_centerline[wake_centerline_mask]\n",
    "    x_wake_centerline = x_centerline[wake_centerline_mask]\n",
    "    \n",
    "    # Count zero crossings (sign changes) in wake\n",
    "    sign_changes = np.sum(np.diff(np.sign(u_wake_centerline)) != 0)\n",
    "    metrics['wake_sign_changes'] = int(sign_changes)\n",
    "    \n",
    "    # 4. Recirculation length estimation\n",
    "    # Find where u becomes positive again after the cylinder\n",
    "    recirculation_length = 0\n",
    "    for i, x_val in enumerate(x_wake_centerline):\n",
    "        if u_wake_centerline[i] > 0.01:  # threshold for positive flow\n",
    "            recirculation_length = x_val - 0.6  # distance from cylinder rear\n",
    "            break\n",
    "    metrics['recirculation_length'] = float(recirculation_length)\n",
    "    \n",
    "    # 5. Wake asymmetry metric\n",
    "    # Compare upper and lower wake regions\n",
    "    upper_wake_mask = ((x_grid >= wake_x_start) & (x_grid <= wake_x_end) & \n",
    "                       (y_grid >= wake_y_center) & (y_grid <= wake_y_center + wake_y_half_width))\n",
    "    lower_wake_mask = ((x_grid >= wake_x_start) & (x_grid <= wake_x_end) & \n",
    "                       (y_grid >= wake_y_center - wake_y_half_width) & (y_grid <= wake_y_center))\n",
    "    \n",
    "    u_upper = np.mean(u[upper_wake_mask])\n",
    "    u_lower = np.mean(u[lower_wake_mask])\n",
    "    metrics['wake_asymmetry'] = float(abs(u_upper - u_lower))\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "# Apply to all your models\n",
    "def analyze_all_models():\n",
    "    \"\"\"Analyze all saved models with enhanced metrics\"\"\"\n",
    "    \n",
    "    # Load the training summary\n",
    "    with open('model_data/track_sep_losses/all_training_summary_2.json', 'r') as f:\n",
    "        results = json.load(f)\n",
    "    \n",
    "    enhanced_results = []\n",
    "    \n",
    "    for result in results:\n",
    "        model_path = result['model_filename']\n",
    "        loss_weights = result['loss_weights']\n",
    "        cyl_weight = loss_weights[-1]  # Last index is cylinder boundary weight\n",
    "        \n",
    "        print(f\"Analyzing model with cylinder weight: {cyl_weight}\")\n",
    "        \n",
    "        # Load model and predict\n",
    "        # from tensorflow.keras.models import load_model # tf.keras.models.load_model\n",
    "        network = tf.keras.models.load_model(model_path)\n",
    "        \n",
    "        # Create inference grid\n",
    "        x = np.linspace(0, 2, 300)\n",
    "        y = np.linspace(0, 1, 300)\n",
    "        x_grid, y_grid = np.meshgrid(x, y)\n",
    "        xy_flat = np.stack([x_grid.flatten(), y_grid.flatten()], axis=-1)\n",
    "        \n",
    "        # Predict\n",
    "        u_v_p = network.predict(xy_flat, batch_size=len(xy_flat))\n",
    "        u = u_v_p[..., 0].reshape(x_grid.shape)\n",
    "        v = u_v_p[..., 1].reshape(x_grid.shape)\n",
    "        p = u_v_p[..., 2].reshape(x_grid.shape)\n",
    "        \n",
    "        # Calculate enhanced metrics\n",
    "        enhanced_metrics = enhanced_wake_metrics(x_grid, y_grid, u, v, p, \n",
    "                                               model_name=f\"cyl_weight_{cyl_weight}\")\n",
    "        \n",
    "        # Combine with existing results\n",
    "        enhanced_result = result.copy()\n",
    "        enhanced_result.update(enhanced_metrics)\n",
    "        enhanced_results.append(enhanced_result)\n",
    "    \n",
    "    return enhanced_results\n",
    "\n",
    "# Run the analysis\n",
    "enhanced_results = analyze_all_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c14c537",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_component_losses():\n",
    "    \"\"\"Analyze how cylinder boundary loss changes with weight\"\"\"\n",
    "    \n",
    "    with open('model_data/track_sep_losses/all_training_summary_2.json', 'r') as f:\n",
    "        results = json.load(f)\n",
    "    \n",
    "    loss_analysis = []\n",
    "    \n",
    "    for result in results:\n",
    "        loss_weights = result['loss_weights']\n",
    "        cyl_weight = loss_weights[-1]  # Cylinder boundary weight\n",
    "        model_name = result['model_filename'].split('/')[-1].replace('.keras', '')\n",
    "        \n",
    "        # Load component losses\n",
    "        component_loss_file = f\"model_data/track_sep_losses/eqn{int(loss_weights[0])}_cyl{int(cyl_weight)}/{model_name}_component_losses.npy\"\n",
    "        \n",
    "        try:\n",
    "            component_losses = np.load(component_loss_file, allow_pickle=True)\n",
    "            \n",
    "            # Extract cylinder boundary loss (last component, index -1)\n",
    "            cyl_losses = []\n",
    "            for iteration_losses in component_losses:\n",
    "                if iteration_losses is not None and len(iteration_losses) > 0:\n",
    "                    cyl_losses.append(iteration_losses[-1])  # Last component is cylinder\n",
    "            \n",
    "            if cyl_losses:\n",
    "                initial_cyl_loss = cyl_losses[0]\n",
    "                final_cyl_loss = cyl_losses[-1]\n",
    "                min_cyl_loss = min(cyl_losses)\n",
    "                \n",
    "                reduction_ratio = (initial_cyl_loss - final_cyl_loss) / initial_cyl_loss\n",
    "                \n",
    "                loss_analysis.append({\n",
    "                    'cyl_weight': cyl_weight,\n",
    "                    'initial_cyl_loss': initial_cyl_loss,\n",
    "                    'final_cyl_loss': final_cyl_loss,\n",
    "                    'min_cyl_loss': min_cyl_loss,\n",
    "                    'loss_reduction_ratio': reduction_ratio,\n",
    "                    'cyl_loss_history': cyl_losses\n",
    "                })\n",
    "        \n",
    "        except FileNotFoundError:\n",
    "            print(f\"Component loss file not found for {model_name}\")\n",
    "    \n",
    "    return loss_analysis\n",
    "\n",
    "# Run component loss analysis\n",
    "loss_analysis = analyze_component_losses()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f5922e3e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'enhanced_results' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-ac737e0ec125>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     68\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[1;31m# Create the comprehensive analysis\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 70\u001b[1;33m \u001b[0mcreate_comprehensive_analysis_plots\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menhanced_results\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_analysis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'enhanced_results' is not defined"
     ]
    }
   ],
   "source": [
    "def create_comprehensive_analysis_plots(enhanced_results, loss_analysis):\n",
    "    \"\"\"Create comprehensive plots for model comparison\"\"\"\n",
    "    \n",
    "    # Extract data for plotting\n",
    "    cyl_weights = [r['loss_weights'][-1] for r in enhanced_results]\n",
    "    \n",
    "    # Create subplots\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    \n",
    "    # 1. Wake metrics vs cylinder weight\n",
    "    min_u_wake = [r['min_u_wake'] for r in enhanced_results]\n",
    "    max_vorticity = [r['max_vorticity_magnitude'] for r in enhanced_results]\n",
    "    recirculation_length = [r['recirculation_length'] for r in enhanced_results]\n",
    "    \n",
    "    axes[0, 0].plot(cyl_weights, min_u_wake, 'bo-', linewidth=2, markersize=8)\n",
    "    axes[0, 0].set_xlabel('Cylinder Boundary Weight')\n",
    "    axes[0, 0].set_ylabel('Min U Velocity in Wake')\n",
    "    axes[0, 0].set_title('Wake Velocity vs Boundary Weight')\n",
    "    axes[0, 0].grid(True)\n",
    "    \n",
    "    axes[0, 1].plot(cyl_weights, max_vorticity, 'ro-', linewidth=2, markersize=8)\n",
    "    axes[0, 1].set_xlabel('Cylinder Boundary Weight')\n",
    "    axes[0, 1].set_ylabel('Max Vorticity Magnitude')\n",
    "    axes[0, 1].set_title('Vorticity vs Boundary Weight')\n",
    "    axes[0, 1].grid(True)\n",
    "    \n",
    "    axes[0, 2].plot(cyl_weights, recirculation_length, 'go-', linewidth=2, markersize=8)\n",
    "    axes[0, 2].set_xlabel('Cylinder Boundary Weight')\n",
    "    axes[0, 2].set_ylabel('Recirculation Length')\n",
    "    axes[0, 2].set_title('Recirculation Length vs Boundary Weight')\n",
    "    axes[0, 2].grid(True)\n",
    "    \n",
    "    # 2. Component loss analysis\n",
    "    if loss_analysis:\n",
    "        loss_cyl_weights = [la['cyl_weight'] for la in loss_analysis]\n",
    "        final_cyl_losses = [la['final_cyl_loss'] for la in loss_analysis]\n",
    "        reduction_ratios = [la['loss_reduction_ratio'] for la in loss_analysis]\n",
    "        \n",
    "        axes[1, 0].plot(loss_cyl_weights, final_cyl_losses, 'mo-', linewidth=2, markersize=8)\n",
    "        axes[1, 0].set_xlabel('Cylinder Boundary Weight')\n",
    "        axes[1, 0].set_ylabel('Final Cylinder Loss')\n",
    "        axes[1, 0].set_title('Cylinder Boundary Loss vs Weight')\n",
    "        axes[1, 0].set_yscale('log')\n",
    "        axes[1, 0].grid(True)\n",
    "        \n",
    "        axes[1, 1].plot(loss_cyl_weights, reduction_ratios, 'co-', linewidth=2, markersize=8)\n",
    "        axes[1, 1].set_xlabel('Cylinder Boundary Weight')\n",
    "        axes[1, 1].set_ylabel('Loss Reduction Ratio')\n",
    "        axes[1, 1].set_title('Cylinder Loss Reduction vs Weight')\n",
    "        axes[1, 1].grid(True)\n",
    "        \n",
    "        # Loss convergence curves\n",
    "        for la in loss_analysis:\n",
    "            iterations = range(len(la['cyl_loss_history']))\n",
    "            axes[1, 2].plot(iterations, la['cyl_loss_history'], \n",
    "                           label=f\"Weight {la['cyl_weight']}\", linewidth=2)\n",
    "        \n",
    "        axes[1, 2].set_xlabel('Iteration')\n",
    "        axes[1, 2].set_ylabel('Cylinder Boundary Loss')\n",
    "        axes[1, 2].set_title('Cylinder Loss Convergence')\n",
    "        axes[1, 2].set_yscale('log')\n",
    "        axes[1, 2].legend()\n",
    "        axes[1, 2].grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('comprehensive_model_analysis.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "# Create the comprehensive analysis\n",
    "create_comprehensive_analysis_plots(enhanced_results, loss_analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f7dc564",
   "metadata": {},
   "outputs": [],
   "source": [
    "def karman_vortex_score(x_grid, y_grid, u, v):\n",
    "    \"\"\"\n",
    "    Calculate a score indicating likelihood of Kármán vortex formation\n",
    "    Higher score = more likely to have vortex shedding\n",
    "    \"\"\"\n",
    "    # Calculate vorticity\n",
    "    dy = y_grid[1, 0] - y_grid[0, 0]\n",
    "    dx = x_grid[0, 1] - x_grid[0, 0]\n",
    "    \n",
    "    du_dy = np.gradient(u, dy, axis=0)\n",
    "    dv_dx = np.gradient(v, dx, axis=1)\n",
    "    vorticity = dv_dx - du_dy\n",
    "    \n",
    "    # Define wake region\n",
    "    wake_mask = ((x_grid >= 0.6) & (x_grid <= 1.5) & \n",
    "                 (y_grid >= 0.2) & (y_grid <= 0.8))\n",
    "    \n",
    "    vorticity_wake = vorticity[wake_mask]\n",
    "    \n",
    "    # Score components:\n",
    "    # 1. Vorticity strength\n",
    "    vorticity_score = np.std(vorticity_wake) * 10\n",
    "    \n",
    "    # 2. Alternating pattern (check for positive and negative vorticity regions)\n",
    "    positive_vorticity = np.sum(vorticity_wake > 0.1)\n",
    "    negative_vorticity = np.sum(vorticity_wake < -0.1)\n",
    "    alternating_score = min(positive_vorticity, negative_vorticity) / 100\n",
    "    \n",
    "    # 3. Recirculation strength\n",
    "    recirculation_score = abs(np.min(u[wake_mask])) * 10\n",
    "    \n",
    "    # Combined score\n",
    "    karman_score = vorticity_score + alternating_score + recirculation_score\n",
    "    \n",
    "    return {\n",
    "        'karman_score': float(karman_score),\n",
    "        'vorticity_component': float(vorticity_score),\n",
    "        'alternating_component': float(alternating_score),\n",
    "        'recirculation_component': float(recirculation_score)\n",
    "    }\n",
    "\n",
    "# Add Kármán score to your analysis\n",
    "def add_karman_scores(enhanced_results):\n",
    "    \"\"\"Add Kármán vortex scores to existing results\"\"\"\n",
    "    for result in enhanced_results:\n",
    "        # Re-load and analyze the model (you might want to cache this)\n",
    "        # ... (similar to previous loading code)\n",
    "        karman_metrics = karman_vortex_score(x_grid, y_grid, u, v)\n",
    "        result.update(karman_metrics)\n",
    "    \n",
    "    return enhanced_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "419bbd49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_summary_report(enhanced_results, loss_analysis):\n",
    "    \"\"\"Generate a comprehensive summary report\"\"\"\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(\"COMPREHENSIVE MODEL ANALYSIS REPORT\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Sort by cylinder weight\n",
    "    enhanced_results_sorted = sorted(enhanced_results, key=lambda x: x['loss_weights'][-1])\n",
    "    \n",
    "    print(f\"{'Cyl Weight':<12} {'Min U Wake':<12} {'Max Vorticity':<15} {'Recirc Length':<15} {'Kármán Score':<12}\")\n",
    "    print(\"-\"*70)\n",
    "    \n",
    "    for result in enhanced_results_sorted:\n",
    "        cyl_weight = result['loss_weights'][-1]\n",
    "        min_u = result['min_u_wake']\n",
    "        max_vort = result.get('max_vorticity_magnitude', 0)\n",
    "        recirc_len = result.get('recirculation_length', 0)\n",
    "        karman_score = result.get('karman_score', 0)\n",
    "        \n",
    "        print(f\"{cyl_weight:<12.1f} {min_u:<12.4f} {max_vort:<15.4f} {recirc_len:<15.4f} {karman_score:<12.4f}\")\n",
    "    \n",
    "    # Find best model for vortex formation\n",
    "    if 'karman_score' in enhanced_results[0]:\n",
    "        best_model = max(enhanced_results, key=lambda x: x['karman_score'])\n",
    "        print(f\"\\nBest model for Kármán vortices: Cylinder weight = {best_model['loss_weights'][-1]}\")\n",
    "        print(f\"Kármán score: {best_model['karman_score']:.4f}\")\n",
    "    \n",
    "    # Component loss analysis summary\n",
    "    if loss_analysis:\n",
    "        print(f\"\\n{'Cyl Weight':<12} {'Initial Loss':<15} {'Final Loss':<15} {'Reduction %':<12}\")\n",
    "        print(\"-\"*60)\n",
    "        for la in sorted(loss_analysis, key=lambda x: x['cyl_weight']):\n",
    "            reduction_pct = la['loss_reduction_ratio'] * 100\n",
    "            print(f\"{la['cyl_weight']:<12.1f} {la['initial_cyl_loss']:<15.6f} {la['final_cyl_loss']:<15.6f} {reduction_pct:<12.2f}\")\n",
    "\n",
    "# Generate the report\n",
    "generate_summary_report(enhanced_results, loss_analysis)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pinnpy3.6",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
